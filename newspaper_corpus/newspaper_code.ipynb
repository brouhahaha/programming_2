{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re           \n",
    "import os\n",
    "import html\n",
    "\n",
    "   #коллекция регулярных выражений\n",
    "    \n",
    "reg_categorie = re.compile('leaf.*?/categories/.*?.html', flags= re.DOTALL)\n",
    "reg_page = re.compile('last last.*?page=\\w\\w?', flags= re.DOTALL)\n",
    "reg_page_number = re.compile('last last.*?page=', flags= re.DOTALL)\n",
    "reg_all_articles_on_page = re.compile ('<div id=\"main-wrap.*?class=\"pager_wrap\">', flags=re.DOTALL)\n",
    "reg_article = re.compile ('href=\"/articles/.*?[\\.\"]', flags=re.DOTALL)\n",
    "reg_text = re.compile ('<div class=\"content\">.*?<div class=\"block block-vkaz\" id=\"block-vkaz-vkaz_share42\">', flags = re.DOTALL)\n",
    "reg_auth = re.compile ('<div class=\"author heading--meta\">.*?</div>', flags = re.DOTALL)         #регулярки для мета данных\n",
    "reg_header = re.compile ('<h1 class=\"title title-story\">.*?</h1>', flags = re.DOTALL)\n",
    "reg_date = re.compile ('<div class=\"submitted heading--meta\">\\d\\d\\.\\d\\d\\.\\d\\d', flags = re.DOTALL)\n",
    "reg_cat = re.compile ('<a href=\".*?\" rel=\"tag\" title=\"\">.*?</a>')\n",
    "\n",
    "main_directory = os.getcwd()            #директория, в которой лежит программа\n",
    "mystem_directory = r\"C:\\mystem.exe \"    #директория, в которой лежит mystem + пробел (он нужен в запросе к майстему)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_url (pageUrl):              #открывает страницу по заданному url\n",
    "    import urllib.request  \n",
    "    req = urllib.request.Request(pageUrl)\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        html = response.read().decode('utf-8')\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_obj (html, regex):      # ищет url по заданному регулярному выражению\n",
    "    found = regex.findall(html)\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_tag (regex, obj_to_clean):         #очищает найденные строки от ненужного мусора, заданного рег.\n",
    "    obj_to_add = re.sub (regex,'', obj_to_clean)\n",
    "    return obj_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def real_urls (objects_to_add, regex):    #склеивает \"чистый\" юрл и имя сайта, выдаеn список юрл-ов\n",
    "    objects_to_url =[]        #что надо убрать из объекта - в регэкс\n",
    "    real_urls =[]\n",
    "    for obj in objects_to_add:\n",
    "        objects_to_url.append(clean_tag(regex, obj))\n",
    "    for obj_to_url in objects_to_url:\n",
    "        real_urls.append(default_Url+obj_to_url)\n",
    "    return real_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_pages (cat_url):                   #считает кол-во страниц в разделе\n",
    "    html = open_url (cat_url)\n",
    "    number_of_pages_is_here = find_obj (html, reg_page)\n",
    "    number_of_pages = clean_tag (reg_page_number, number_of_pages_is_here[0])\n",
    "    return int(number_of_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def articles (cat_url):                              #собирает массив с юрлами статей\n",
    "    html = open_url (cat_url)\n",
    "    code_where_articles_urls_are = find_obj (html, reg_all_articles_on_page)\n",
    "    articles = find_obj (code_where_articles_urls_are[0], reg_article)\n",
    "    articles_urls = list(set(real_urls (articles, 'href=\"/')))\n",
    "    new_articles= []\n",
    "    for article in articles_urls:\n",
    "        new_articles.append(article.strip('\\.\"')+'.html')\n",
    "    return new_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def article_content (article_url):      #достает и очищает содержимое страницы со статьей\n",
    "    text_code = open_url (article_url)\n",
    "    text_is_here = find_obj (text_code, reg_text)\n",
    "    text = clean_tag ('<.*?>', text_is_here[0])\n",
    "    text = html.unescape(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_directory (newspaper, folder, year, month):                   #создает директорию\n",
    "    path = main_directory\n",
    "    #r'C:\\Users\\Maria\\programming_2'\n",
    "    directory = os.path.join (path, newspaper, folder, year, month)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    return directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_meta_data (page_url):                                 #получает мета-данные из статьи, возвращает словарь с ними\n",
    "    html = open_url (page_url)\n",
    "    metadata = {'path':'', 'author':'','sex':'','birthday':'','header':'', 'created':'', 'sphere':'публицистика', 'genre_fi':'','type':'','topic':'','chronotop ':'','style':'нейтральный',\n",
    "'audience_age' :'н-возраст', 'audience_level': 'н-уровень','audience_size' :'городская','source' :'','publication':'Вечерняя Казань','publisher' :'',\n",
    "'publ_year' :'','medium' : 'газета','country' : 'Россия','region' : 'Республика Татарстан','language': 'ru' }\n",
    "    #path =  путь к неразмеченному файлу, достается в главной функции\n",
    "    find_author = find_obj (html, reg_auth)\n",
    "    metadata ['author'] = re.sub('<.*?>', '',find_author [0])\n",
    "    find_header = find_obj (html, reg_header)\n",
    "    metadata ['header'] = re.sub('<.*?>', '',find_header [0])\n",
    "    find_date = find_obj (html, reg_date)\n",
    "    metadata ['created'] = re.sub('<.*?>', '',find_date [0])\n",
    "    find_categorie = find_obj (html, reg_cat)\n",
    "    metadata ['topic'] = re.sub('<.*?>', '',find_categorie [0])\n",
    "    metadata ['source'] = page_url\n",
    "    metadata ['publ_year'] = '20'+re.sub('<.*?>\\d\\d\\.\\d\\d\\.', '',find_date [0])\n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_directory_and_go(metadata):                             #создает конкретную директорию и переходит в неё              \n",
    "    make_directory ('newspaper','plain','20'+metadata['created'][6:8] , metadata['created'][3:5])\n",
    "    os.chdir (os.path.join(main_directory,'newspaper','plain','20'+metadata['created'][6:8] , metadata['created'][3:5]))\n",
    "    return os.path.join(main_directory,'newspaper','plain','20'+metadata['created'][6:8] , metadata['created'][3:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def article_into_file (i, metadata, article):                      #записывает содержимое статьи в файл\n",
    "    with open(str(i)+'.txt', 'w', encoding='utf-8') as g:\n",
    "        article_txt = '@au '+metadata['author']+'\\n'+'@ti '+metadata['header']+'\\n'+'@da '+metadata['created']+'\\n'+'@topic '+metadata['topic']+'\\n'+'@url '+metadata['source']+'\\n'\n",
    "        g.write(article_txt+article_content(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv                                                        #функции для создания csv с метаданными\n",
    "\n",
    "def create_meta_csv (metadata):                                    #создаётся таблица\n",
    "    with open('metadata.csv', \"w\", newline=\"\") as file:\n",
    "        columns = metadata.keys()\n",
    "        writer = csv.DictWriter(file, delimiter = '\\t', fieldnames=columns)\n",
    "        writer.writeheader()\n",
    "    \n",
    "def add_data_to_csv (metadata):                                #добавляются данные конкретной статьи в таблицу\n",
    "    with open('metadata.csv', \"a\", newline=\"\") as file:\n",
    "        columns = metadata.keys()\n",
    "        writer = csv.DictWriter(file, delimiter = '\\t', fieldnames=columns)\n",
    "        writer.writerow(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mystem_processing(folder_name, metadata, format_type, mode, i):              # обработка текста майстемом \n",
    "    outputdir = make_directory ('newspaper', folder_name,'20'+metadata['created'][6:8] , metadata['created'][3:5])\n",
    "    metadata ['path'] = create_directory_and_go(metadata)+ os.sep +str(i)+'.txt'\n",
    "    fl_dir = metadata['path']+' '\n",
    "    os.system(mystem_directory + fl_dir + outputdir + os.sep + str(i) + ' '+ format_type +\" \"+ mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "def main_func (default_Url):            #основная функция\n",
    "    html = open_url (default_Url)\n",
    "    categories = find_obj (html, reg_categorie)\n",
    "    categories_urls = real_urls (categories, 'leaf.*?/') \n",
    "    i = 0\n",
    "    for cat_url in categories_urls:\n",
    "        pages = count_pages (cat_url)\n",
    "        for page in range(pages):\n",
    "            page_url = cat_url+'?page='+str(page)\n",
    "            articles_urls = articles (page_url)\n",
    "            try:\n",
    "                for article in articles_urls:\n",
    "                    print (article)\n",
    "                    metadata = get_meta_data (article)\n",
    "                    metadata ['path'] = create_directory_and_go(metadata)+str(i)+'.txt'\n",
    "                    article_into_file(i, metadata, article)\n",
    "                    os.chdir (os.path.join(main_directory,'newspaper'))\n",
    "                    if i == 0:\n",
    "                        create_meta_csv (metadata)\n",
    "                    else:\n",
    "                        pass\n",
    "                    add_data_to_csv(metadata)\n",
    "                    mystem_processing ('mystem-xml', metadata, '--format xml', '-di', i)\n",
    "                    mystem_processing ('mystem-plain', metadata, '--format text', '-di', i)\n",
    "                    i+=1\n",
    "            except urllib.error.HTTPError: \n",
    "                pass\n",
    "            except UnicodeEncodeError:\n",
    "                pass\n",
    "                \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_Url = 'http://www.evening-kazan.ru/'\n",
    "main_func (default_Url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
